{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".reveal {\n",
       "overflow: visible;\n",
       "}\n",
       "\n",
       "/*********************************************\n",
       " * COLORS FOR EXAMPLES\n",
       " *********************************************/\n",
       "em {font-style: normal !important;\n",
       "    color: #800000;}\n",
       "span.good {color: #008000;}\n",
       "span.warning {color: #808000;}\n",
       "span.bad {color: #800000;}\n",
       "\n",
       "/*********************************************\n",
       " * GLOBAL STYLES\n",
       " *********************************************/\n",
       ".reveal h1 {color: #000000; text-shadow: 0px 0px 6px rgba(0, 0, 0, 0.2);}\n",
       ".reveal h2 {color: #222222; text-shadow: 0px 0px 5px rgba(0, 0, 0, 0.2);}\n",
       ".reveal h3 {color: #444444; text-shadow: 0px 0px 4px rgba(0, 0, 0, 0.2);}\n",
       ".reveal h4 {color: #666666; text-shadow: 0px 0px 3px rgba(0, 0, 0, 0.2);}\n",
       ".reveal h5 {color: #888888; text-shadow: 0px 0px 2px rgba(0, 0, 0, 0.2);}\n",
       ".reveal h6 {color: #AAAAAA; text-shadow: 0px 0px 1px rgba(0, 0, 0, 0.2);}\n",
       "\n",
       "/*********************************************\n",
       " * IMAGES\n",
       " *********************************************/\n",
       ".reveal section img { margin-left:auto; margin-right:auto;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "IPython Notebook v4.0 para python 2.7\n",
    "Librerías adicionales: numpy, matplotlib\n",
    "Contenido bajo licencia CC-BY 4.0. Código bajo licencia MIT. (c) Sebastian Flores.\n",
    "\"\"\"\n",
    "\n",
    "# Configuracion para recargar módulos y librerías \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(open(\"style/mat281.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<header class=\"w3-container w3-teal\">\n",
    "<img src=\"images/utfsm.png\" alt=\"\" height=\"100px\" align=\"left\"/>\n",
    "<img src=\"images/mat.png\" alt=\"\" height=\"100px\" align=\"right\"/>\n",
    "</header>\n",
    "<br/><br/><br/><br/><br/>\n",
    "# MAT281\n",
    "## Aplicaciones de la Matemática en la Ingeniería\n",
    "### Sebastián Flores\n",
    "https://www.github.com/usantamaria/mat281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clase anterior\n",
    "\n",
    "***Clustering***\n",
    "* ¿Cómo se llamaba el algoritmo que vimos? \n",
    "* ¿Cuándo funcionaba y cuándo fallaba?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué veremos hoy?\n",
    "Regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Porqué veremos ese contenido?\n",
    "Porque regresión lineal es universalmente utilizado, y la derivación del método nos entrega importantes consideraciones sobre su implementación, sus hipótesis y sus posibles extensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo 2D\n",
    "Consideremos los siguientes datos:\n",
    "[link](http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head data/x01.txt -n 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Plot of data\n",
    "data = np.loadtxt(\"data/x01.txt\", skiprows=33)\n",
    "x = data[:,1]\n",
    "y = data[:,2]\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(x, y, 'rs')\n",
    "plt.xlabel(\"brain weight\")\n",
    "plt.ylabel(\"body weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Plot of data\n",
    "data = np.loadtxt(\"data/x01.txt\", skiprows=33)\n",
    "x = np.log(data[:,1])\n",
    "y = np.log(data[:,2])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(x, y, 'rs')\n",
    "plt.xlabel(\"log brain weight\")\n",
    "plt.ylabel(\"log body weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo 3D\n",
    "Consideremos los siguientes datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head data/x06.txt -n 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib gtk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# Plot of data\n",
    "data = np.loadtxt(\"data/x06.txt\", skiprows=37)\n",
    "x = data[:,1]\n",
    "y = data[:,2]\n",
    "z = data[:,3]\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, 'rs')\n",
    "plt.xlabel(\"age [days]\")\n",
    "plt.ylabel(\"Water Temperature [C]\")\n",
    "plt.title(\"Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# Plot of data\n",
    "data = np.loadtxt(\"data/x06.txt\", skiprows=37)\n",
    "x = np.log(data[:,1])\n",
    "y = np.log(data[:,2])\n",
    "z = np.log(data[:,3])\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, 'rs')\n",
    "plt.xlabel(\"age [days]\")\n",
    "plt.ylabel(\"Water Temperature [C]\")\n",
    "plt.title(\"Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Los datos\n",
    "\n",
    "Supondremos que tenemos $m$ datos. \n",
    "Cada dato $x^{(i)}$, $i=1,\\dots,$ $m$ tiene $n$ componentes,\n",
    "$x^{(i)} = (x^{(i)}_1, ..., x^{(i)}_n)$. \n",
    "\n",
    "Conocemos además el valor (etiqueta) asociado a $x^{(i)}$ que llamaremos $y^{(i)}$, $i=1,\\dots, m$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Nuestra hipótesis de modelo lineal puede escribirse como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_{\\theta}(x) &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_1 x_n \\\\\n",
    "          &= \\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n\\end{bmatrix} \\begin{bmatrix}1 \\\\ x_1 \\\\x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\\\\n",
    "          &= \\theta^T \\begin{bmatrix}1\\\\x\\end{bmatrix} = \\begin{bmatrix}1 & x^T\\end{bmatrix} \\theta \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Definiremos $x^{(i)}_0 =1$, de modo que\n",
    "$h_{\\theta}(x^{(i)}) = (x^{(i)})^T \\theta $ y buscamos el vector de parámetros\n",
    "$$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "Definamos las matrices\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Y &= \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "1 & x^{(1)}_1 & \\dots & x^{(1)}_n \\\\ \n",
    "1 & x^{(2)}_1 & \\dots & x^{(2)}_n \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x^{(m)}_1 & \\dots & x^{(m)}_n \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix} \n",
    "- (x^{(1)})^T - \\\\ \n",
    "- (x^{(2)})^T - \\\\\n",
    "\\vdots \\\\\n",
    "- (x^{(m)})^T - \\\\\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Luego la evaluación\n",
    "de todos los datos puede escribirse matricialmente como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X \\theta &= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & ... & x_n^{(1)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_1^{(m)} & ... & x_n^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "1 \\theta_0 + x^{(1)}_1 \\theta_1 + ... + x^{(1)}_n \\theta_n \\\\\n",
    "\\vdots \\\\\n",
    "1 \\theta_0 + x^{(m)}_1 \\theta_1 + ... + x^{(m)}_n \\theta_n \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "h(x^{(1)}) \\\\\n",
    "\\vdots \\\\\n",
    "h(x^{(m)})\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Nuestro problema es\n",
    "encontrar un “buen” conjunto de valores $\\theta$ de modo que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "h(x^{(1)}) \\\\\n",
    "h(x^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "h(x^{(m)})\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "es decir, que $$X \\theta \\approx Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Para encontrar el mejor vector $\\theta$ podríamos definir una función de costo $J(\\theta)$ de la siguiente manera:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "El mejor vector $\\theta$ sería aquel que permite minimizar la norma 2 entre la predicción y el valor real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Ingenieril\n",
    "\n",
    "¿Cómo podemos resolver el problema\n",
    "en el menor número de pasos?\n",
    "\n",
    "Deseamos resolver el sistema $$A x = b$$ con\n",
    "$A \\in \\mathbb{R}^{m \\times n}$ y $m > n$ (La matrix $A$ es skinny).\n",
    "\n",
    "¿Cómo resolvemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Ingenieril\n",
    "\n",
    "Bueno,\n",
    "si $A \\in \\mathbb{R}^{m \\times n}$, entonces\n",
    "$A^T \\in \\mathbb{R}^{n \\times m}$ y la multiplicación está bien definida\n",
    "y obtengo un sistema lineal $n \\times n$. $$(A^T A) \\  x = A^T b$$ Si la\n",
    "matriz $A^T A$ es invertible, el sistema se puede solucionar “sin mayor\n",
    "reparo”. $$x = (A^T A)^{-1} A^T b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Ingenieril\n",
    "\n",
    "En\n",
    "nuestro caso, obtendríamos $$\\theta = (X^T X)^{-1} X^T Y$$ Esta\n",
    "respuesta, aunque correcta, no admite interpretaciones y no permite\n",
    "generalizar a otros casos más generales.\n",
    "\n",
    "En particular...\n",
    "\n",
    "-   ¿Qué relación tiene con la función de costo (no) utilizada?\n",
    "\n",
    "-   ¿Qué pasa si $A^T A$ no es invertible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Machine Learning\n",
    "\n",
    "¿Cómo podemos obtener una\n",
    "buena aproximación para $\\theta$?\n",
    "\n",
    "Queremos encontrar $\\theta^*$ que minimice $J(\\theta)$.\n",
    "\n",
    "Basta con utilizar una buena rutina de optimización para cumplir con\n",
    "dicho objetivo.\n",
    "\n",
    "En particular, una elección natural es tomar la dirección de mayor\n",
    "descenso, es decir, el método del máximo descenso (gradient descent).\n",
    "$$\\theta^{(n+1)} = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)})$$\n",
    "donde $\\alpha >0$ es la tasa de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Machine Learning\n",
    "\n",
    "En\n",
    "nuestro caso, puesto que tenemos\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "se tiene que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} &=\n",
    "\\frac{\\partial }{\\partial \\theta_k} \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m}  2 \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_k}  \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Machine Learning\n",
    "\n",
    "Este\n",
    "algoritmo se llama Least Mean Squares\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)}) \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n",
    "\n",
    "OBS: La elección de $\\alpha$ es crucial para la convergencia. En\n",
    "particular, $0.01/m$ funciona bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def lms_regression(X, Y, theta, tol=1E-6):\n",
    "    converged = False\n",
    "    alpha = 0.01/len(Y)\n",
    "    while not converged:\n",
    "        gradient = 0.\n",
    "        for xiT, yi in zip(X,Y):\n",
    "            hi = np.dot(theta, xiT)\n",
    "            gradient += (hi - yi)*xiT.T\n",
    "        new_theta = theta - alpha * gradient\n",
    "        converged = norm(theta-new_theta) < tol * norm(theta)\n",
    "        theta = new_theta\n",
    "    return theta\n",
    "\n",
    "m = 20\n",
    "t = np.linspace(0,1,m)\n",
    "x = 2 + 2*t\n",
    "y = 300 + 100*t\n",
    "X = np.array([np.ones(m), x]).T\n",
    "Y = y.reshape(m,1)\n",
    "theta_0 = np.array([[0.0,0.0]])\n",
    "theta = lms_regression(X, Y, theta_0)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Machine Learning\n",
    "#### Least Mean Squares\n",
    "\n",
    "Resultados:\n",
    "\n",
    "-   Tarda del orden de 4 segundos para un problema ridículamente\n",
    "    pequeño.\n",
    "\n",
    "-   Precisión no es tan buena como esperábamos $\\theta=(199.45, 50.17)$\n",
    "    en vez de $(200, 50)$.\n",
    "\n",
    "-   ¿Hay algo mejor que se pueda hacer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Matemática\n",
    "**¿Cómo podemos obtener una\n",
    "justificación para la ecuación normal?**\n",
    "\n",
    "Necesitamos los siguientes ingredientes:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_x &(x^T A x) = A x + A^T x \\\\ \n",
    "\\nabla_x &(b^T x) = b \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Matemática\n",
    "Se tiene\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) \n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\\\\n",
    "&= \\frac{1}{2} \\left( X \\theta - Y \\right)^T \\left( X \\theta - Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T - Y^T \\right) \\left( X \\theta - Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T X \\theta - \\theta^T X^T Y - Y^T X \\theta + Y^T Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T X \\theta - 2 (Y^T X) \\theta + Y^T Y \\right)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Matemática\n",
    "\n",
    "Aplicando a cada uno de los términos, obtenemos:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( \\theta^T X^T X \\theta ) &= X^T X \\theta + (X^T X)^T \\theta \\\\\n",
    "& = 2 X^T X \\theta\\end{aligned}$$\n",
    "\n",
    "también se tiene\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( Y^T X \\theta  ) &= (Y^T X) ^T\\\\\n",
    "&= X^T Y\\end{aligned}$$\n",
    "\n",
    "y por último\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( Y^T Y  ) = 0\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Matemática\n",
    "\n",
    "Por lo tanto se tiene que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "& = \\nabla_\\theta  \\frac{1}{2} \\left( \\theta^T X^T X \\theta - 2 (Y^T X) \\theta + Y^T Y \\right) \\\\\n",
    "&= \\frac{1}{2} ( 2 X^T X \\theta - 2 X^T Y + 0 ) \\\\\n",
    "&= X^T X \\theta - X^T Y \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Matemática\n",
    "\n",
    "Esto significa que el problema $$\\min_\\theta J(\\theta)$$ se resuelve al\n",
    "hacer todas las derivadas parciales iguales a cero (ie, gradiente igual\n",
    "a cero) $$\\nabla_\\theta J(\\theta) = 0$$ lo cual en nuestro caso se\n",
    "convierte convenientemente a la ecuación normal $$X^T X \\theta = X^T Y$$\n",
    "y se tiene $$\\theta = (X^T X)^{-1} X^T Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def matrix_regression(X, Y, theta, tol=1E-6):\n",
    "    A = np.dot(X.T,X)\n",
    "    b = np.dot(X.T,Y)\n",
    "    sol = np.linalg.solve(A,b)\n",
    "    return sol.flatten()\n",
    "\n",
    "m = 20\n",
    "t = np.linspace(0,1,m)\n",
    "x = 2 + 2*t\n",
    "y = 300 + 100*t\n",
    "X = np.array([np.ones(m), x]).T\n",
    "Y = y.reshape(m,1)\n",
    "theta_0 = np.array([[0.0,0.0]])\n",
    "theta = matrix_regression(X, Y, theta_0)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aproximación Matemática\n",
    "\n",
    "Resultados:\n",
    "\n",
    "-   Tarda mucho menos que LMS para un problema pequeño.\n",
    "\n",
    "-   Precisión es buena: $(200, 50)$.\n",
    "\n",
    "-   Problema potencial es tener suficiente memoria RAM: $X^T X$ puede ser una matriz costosa de conseguir, aunque es sólo de tamaño $n\\times n$, con $n$ el tamaño del vector $\\theta$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "#### ¿Porqué la función de costo $J(\\theta)$ en norma $2$ resulta adecuada?\n",
    "\n",
    "Asumamos que outputs e inputs están relacionados mediante\n",
    "$$y^{(i)}= \\theta^T x^{(i)}+ \\varepsilon^{(i)}$$ donde\n",
    "$\\varepsilon^{(i)}$ es un error que captura efectos sin modelar o ruido\n",
    "de medición. \n",
    "\n",
    "Supongamos que los $\\varepsilon^{(i)}$ se distribuyen\n",
    "de manera idéntica e independientemente de acuerdo a una distribución\n",
    "gausiana de media $0$ y varianza $\\sigma^2$.\n",
    "$$\\varepsilon^{(i)}\\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "Cabe destacar que:\n",
    "\n",
    "-   $\\theta$ no es una variable aleatoria, es un parámetro\n",
    "    (desconocido).\n",
    "\n",
    "-   $\\varepsilon^{(i)}$, $x^{(i)}$ y $y^{(i)}$ son variables aleatorias.\n",
    "\n",
    "-   $\\varepsilon^{(i)}\\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "-   $y^{(i)} \\ | \\ x^{(i)}; \\theta \\sim \\mathcal{N}(\\theta^T x^{(i)}, \\sigma^2)$\n",
    "    pues $y^{(i)}= \\theta^T x^{(i)}+ \\varepsilon^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "Tenemos entonces que\n",
    "$$\\mathbb{P}[\\varepsilon^{(i)}] = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2} \\right)$$\n",
    "y por tanto\n",
    "$$\\mathbb{P}[y^{(i)}\\ | \\ x^{(i)}; \\theta ] = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(y^{(i)}- \\theta^T x^{(i)})^2}{2\\sigma^2} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "La función de verosimilitud $L(\\theta)$ nos\n",
    "permite entender que tan probable es encontrar los datos observados,\n",
    "para una elección del parámetro $\\theta$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(\\theta) \n",
    "&= \\prod_{i=1}^{m} \\mathbb{P}[y^{(i)}| x^{(i)}; \\theta ] \\\\\n",
    "&= \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(y^{(i)}- \\theta^T x^{(i)})^2}{2\\sigma^2} \\right) \\\\\n",
    "&= \\frac{1}{(\\sqrt{2\\pi}\\sigma)^{m}} \\exp\\left( - \\sum_{i=1}^{m}  \\frac{(y^{(i)}- \\theta^T x^{(i)})^2}{2\\sigma^2} \\right)\\end{aligned}$$\n",
    "\n",
    "Nos gustaría encontrar el parámetro $\\theta$ que más probablemente haya\n",
    "generado los datos observados, es decir, el parámetro $\\theta$ que\n",
    "maximiza la función de verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "-   Maximizar la verosimilitud $L(\\theta)$ es lo mismo que maximizar la\n",
    "    función de log-verosimitud $l(\\theta)=\\log(L(\\theta))$ puesto que\n",
    "    $\\log$ es una función monótonamente creciente.\n",
    "\n",
    "-   Maximizar $-f(\\theta)$ es lo mismo que minimizar $f(\\theta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretación Probabilística\n",
    "\n",
    "$$\\begin{aligned}\n",
    "l(\\theta) \n",
    "&= \\log( L(\\theta) \\\\\n",
    "&= \\log\\left[ \\frac{1}{(\\sqrt{2\\pi}\\sigma)^{m}} \\exp\\left( - \\sum_{i=1}^{m}  \\frac{(y^{(i)}- \\theta^T x^{(i)})^2}{2\\sigma^2} \\right) \\right] \\\\\n",
    "&= - m \\log (\\sqrt{2\\pi} \\sigma) - \\frac{1}{\\sigma^2} \\frac{1}{2} \\sum_{i=1}^{m} \\left( y^{(i)} - \\theta^T x^{(i)}\\right)^2 \\end{aligned}$$\n",
    "\n",
    "Es decir, la función costo $J(\\theta)$ cuadrática puede interpretarse\n",
    "como un intento por encontrar el parámetro $\\theta$ más probable bajo la\n",
    "hipótesis que el error en $y$ es gaussiano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aspectos Prácticos\n",
    "\n",
    "#### ¿Cómo se aplica regresión en realidad?\n",
    "\n",
    "Al realizar regresión, conviene normalizar/estandarizar los datos, es\n",
    "decir transformarlos para que tengan una escala común:\n",
    "\n",
    "-   Utilizando la media y la desviación estándar\n",
    "    $$\\frac{x_i-\\overline{x_i}}{\\sigma_{x_i}}$$\n",
    "\n",
    "-   Utilizando mínimos y máximos\n",
    "    $$\\frac{x_i-\\min{x_i}}{\\max{x_i} - \\min{x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aspectos Prácticos\n",
    "\n",
    "#### ¿Porqué normalizar?\n",
    "\n",
    "-   Los valores numéricos poseen escalas de magnitud distintas.\n",
    "\n",
    "-   Las variables tienen distintos significados físicos.\n",
    "\n",
    "-   Algoritmos funcionan mejor.\n",
    "\n",
    "-   Interpretación de resultados es más sencilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo en Boston House-price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# The data\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "# What’s the data\n",
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from mat281_code.lms import lms_regression as lms\n",
    "from sklearn import linear_model\n",
    "# The data\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "m = X.shape[0]\n",
    "\n",
    "# Normalization of data\n",
    "#X_train_aux = (X-X.min(axis=0))/(X.max(axis=0)-X.min(axis=0))\n",
    "#Y_train_aux = (Y-Y.min())/(Y.max()-Y.min())\n",
    "\n",
    "X_train_aux = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "Y_train_aux = (Y-Y.mean())/Y.std()\n",
    "# Put in shape for normal equations\n",
    "X_train = np.hstack([np.ones([m,1]), X_train_aux])\n",
    "Y_train = Y_train_aux.reshape(m,1)\n",
    "\n",
    "# Direct Solution\n",
    "theta = np.linalg.solve(np.dot(X_train.T, X_train), np.dot(X_train.T, Y_train))\n",
    "print theta.flatten()\n",
    "\n",
    "# sklearn solution\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train_aux, Y_train_aux)\n",
    "theta = regr.intercept_, regr.coef_\n",
    "print theta\n",
    "\n",
    "# LMS Solution - Tarda mucho\n",
    "#theta0 = Y_train.mean()/X_train.mean(axis=0)/X.shape[1]\n",
    "#theta = lms(X_train, Y_train, theta0)\n",
    "#print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análisis del ejemplo\n",
    "\n",
    "Las ecuaciones\n",
    "normales y sklearn entregan el mismo resultado:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta = (& 0.00, -0.10, 0.12, 0.02, 0.07, -0.22, \\\\\n",
    "          & 0.29, 0.00, -0.34, 0.29, -0.23, -0.22, 0.09, -0.41 )\\end{aligned}$$\n",
    "\n",
    "Mientras que el algoritmo lms entrega\n",
    "\n",
    "$$\\begin{aligned}\n",
    " \\theta = (&0.00, -0.10, 0.12, 0.02, 0.07, -0.21, \\\\\n",
    "             &0.29, 0.00, -0.34, 0.29, -0.23, -0.21, 0.09, -0.41 )\\end{aligned}$$\n",
    "\n",
    "Si las variables son\n",
    "`CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT, MEDV`\n",
    "\n",
    "¿Cuáles variables tienen más impacto en el precio de la vivienda?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análisis del ejemplo\n",
    "-   $\\theta_0=+0.00$.\n",
    "\n",
    "-   $\\theta_1 = -0.10$: CRIM, per capita crime rate by town.\n",
    "\n",
    "-   $\\theta_2 = +0.12$: ZN, proportion of residential land zoned for\n",
    "    lots over 25,000 sq.ft.\n",
    "\n",
    "-   $\\theta_3 = +0.02$: INDUS, proportion of non-retail business acres\n",
    "    per town\n",
    "\n",
    "-   $\\theta_4 = +0.07$: CHAS, Charles River dummy variable (= 1 if tract\n",
    "    bounds river; 0 otherwise)\n",
    "\n",
    "-   $\\theta_5 = -0.22$: NOX, nitric oxides concentration (parts per 10\n",
    "    million)\n",
    "\n",
    "-   $\\theta_6 = +0.29$: RM, average number of rooms per dwelling\n",
    "\n",
    "-   $\\theta_7 = +0.00$: AGE, proportion of owner-occupied units built\n",
    "    prior to 1940\n",
    "\n",
    "-   $\\theta_8 = -0.34$: DIS, weighted distances to five Boston\n",
    "    employment centres\n",
    "\n",
    "-   $\\theta_9 = +0.29$: RAD, index of accessibility to radial highways\n",
    "\n",
    "-   $\\theta_{10} = -0.23$: TAX, full-value property-tax rate per\n",
    "    \\$10,000\n",
    "\n",
    "-   $\\theta_{11} = -0.22$: PTRATIO pupil-teacher ratio by town\n",
    "\n",
    "-   $\\theta_{12} = +0.09$: B, $1000(Bk - 0.63)^2$ where Bk is the\n",
    "    proportion of blacks by town\n",
    "\n",
    "-   $\\theta_{13} = -0.41$: LSTAT, % lower status of the population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Análisis del ejemplo\n",
    "\n",
    "-   ¿Es posible graficar la solución?\n",
    "\n",
    "-   ¿Cómo sabemos si el modelo es bueno?\n",
    "\n",
    "-   ¿Cuál es el error de entrenamiento? ¿Cuál es el error de predicción?\n",
    "\n",
    "-   ¿Podemos utilizar el modelo para realizar predicciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicación a Iris Dataset\n",
    "\n",
    "Recordemos el Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def plot(dataset, ax, i, j):\n",
    "    ax.scatter(dataset.data[:,i], dataset.data[:,j], c=dataset.target, s=50)\n",
    "    ax.set_xlabel(dataset.feature_names[i], fontsize=20)\n",
    "    ax.set_ylabel(dataset.feature_names[j], fontsize=20)\n",
    "\n",
    "# row and column sharing\n",
    "f, ((ax1, ax2), (ax3, ax4), (ax5,ax6)) = plt.subplots(3, 2, figsize=(16,16))\n",
    "plot(iris, ax1, 0, 1)\n",
    "plot(iris, ax2, 0, 2)\n",
    "plot(iris, ax3, 1, 2)\n",
    "plot(iris, ax4, 0, 3)\n",
    "plot(iris, ax5, 1, 3)\n",
    "plot(iris, ax6, 2, 3)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicación a Iris Dataset\n",
    "\n",
    "Busquemos aplicar una relación lineal a cada clase. Para ello utilizamos 3 atributos para predecir el cuarto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# REVISAR\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Loading the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "iris_label = iris.target\n",
    "\n",
    "# Apply linear regression to each model\n",
    "predictions = {}\n",
    "regr = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "for label in range(0,3):\n",
    "    X_train = X[iris_label==label][:,:-1]\n",
    "    Y_train = X[iris_label==label][:,-1]\n",
    "    regr.fit(X_train, Y_train) # Still must add the column of 1\n",
    "    theta = regr.intercept_, regr.coef_\n",
    "    print theta\n",
    "    Y_pred = regr.predict(X_train)\n",
    "    predictions[label] = Y_pred\n",
    "    print \"Error\", np.linalg.norm(Y_train-Y_pred,2)/len(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def plot(dataset, ax, i, j):\n",
    "    colors  = {0:\"r\", 1:\"b\", 2:\"g\"}\n",
    "    markers = {0:\"s\", 1:\"o\", 2:\"<\"}\n",
    "    for label in range(3):\n",
    "        x = dataset.data[:,i][dataset.target==label]\n",
    "        y = dataset.data[:,j][dataset.target==label]\n",
    "        ax.scatter(x, y, c=colors[label], marker=markers[label], s=50)\n",
    "        if j==3:\n",
    "            ax.scatter(x, predictions[label], c=\"w\", marker=markers[label], \n",
    "                       s=50)\n",
    "    ax.set_xlabel(dataset.feature_names[i], fontsize=20)\n",
    "    ax.set_ylabel(dataset.feature_names[j], fontsize=20)\n",
    "    \n",
    "# row and column sharing\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16,16))\n",
    "plot(iris, ax1, 0, 3)\n",
    "plot(iris, ax2, 1, 3)\n",
    "plot(iris, ax3, 2, 3)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "* Jake VanderPlas, ESAC Data Analysis and Statistics Workshop 2014, https://github.com/jakevdp/ESAC-stats-2014\n",
    "* Andrew Ng, Machine Learning CS144, Stanford University."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
